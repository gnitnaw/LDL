{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe MIT License (MIT)\\nCopyright (c) 2021 NVIDIA\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\nthe Software, and to permit persons to whom the Software is furnished to do so,\\nsubject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example trains an encoder-decoder network to generate textual descriptions of images. The architecture resembles the language translation network in c14e1_seq2seq2_translate, but the encoder is a convolutional network instead of a recurrent network. Additionally, the decoder makes use of attention. The implementation relies on precomputed feature vectors from c16e1_create_vectors. More context for this code example can be found in the section \"Programming Example: Attention-Based Image Captioning\" in Chapter 16 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n",
    "\n",
    "We start with import statements in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 16:35:15.557918: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-19 16:35:15.701910: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-19 16:35:15.702788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-19 16:35:16.493201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import \\\n",
    "    text_to_word_sequence\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import \\\n",
    "    preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import \\\n",
    "    pad_sequences\n",
    "import pickle\n",
    "import gzip\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization statements for our program are found below. They are similar to what we used in the language translation example, but some of the lines deserve further attention. The variable READ_IMAGES can be used to limit the number of images that we use for training. We set it to 90,000, which is more than the total number of images we have. You can decrease it if necessary (e.g., if you run into memory limits of your machine). We also provide the paths to four files that we will use as test images. You can replace those to point to images of your own choice when you run this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_IMAGES = 90000\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "TRAINING_FILE_DIR = 'tf_data/feature_vectors/'\n",
    "TEST_FILE_DIR = '../data/test_images/'\n",
    "TEST_IMAGES = ['boat.jpg',\n",
    "               'cat.jpg',\n",
    "               'table.jpg',\n",
    "               'bird.jpg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the functions we use to read the image captions. The function to read the captions reads the pickled directory file that we previously prepared. From this, we create a list image_paths with the filenames for the feature vectors and one list, dest_word_sequences, which contains the first image caption for each image. To keep things simple, we simply discard the alternative captions for each image.\n",
    "\n",
    "The list dest_word_sequences is equivalent to the destination language sentence in the language translation example. This function does not load all the feature vectors but just the paths to them. The reason for this is that the feature vectors for all the images consume a fair amount of space, so for many machines, it would be impractical to hold the entire dataset in memory during training. Instead, we read the feature vectors on the fly when they are needed. This is a common technique when working with large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read file.\n",
    "def read_training_file(file_name, max_len):\n",
    "    pickle_file = gzip.open(file_name, 'rb')\n",
    "    image_dict = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    image_paths = []\n",
    "    dest_word_sequences = []\n",
    "    for i, key in enumerate(image_dict):\n",
    "        if i == READ_IMAGES:\n",
    "            break\n",
    "        image_item = image_dict[key]\n",
    "        image_paths.append(image_item[0])\n",
    "        caption = image_item[1]\n",
    "        word_sequence = text_to_word_sequence(caption)\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    return image_paths, dest_word_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet contains functions to tokenize and untokenize the sentences. These are similar, if not identical, to what we used in c14e1_seq2seq_translate. We finally call the functions to read and tokenize the image captions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to tokenize and un-tokenize sequences.\n",
    "def tokenize(sequences):\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
    "                          oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts(\n",
    "                [[index]])[0])\n",
    "    print(word_seq)\n",
    "\n",
    "# Read files.\n",
    "image_paths, dest_seq = read_training_file(TRAINING_FILE_DIR \\\n",
    "    + 'caption_file.pickle.gz', MAX_LENGTH)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, we cannot afford to keep the entire dataset in memory during training but need to create our training batches on the fly. We handle this task by creating a class that inherits from the Keras Sequence class in the code snippet below. In the constructor, we supply the paths to the feature vectors, as well as the tokenized captions, and the batch size. Just as for the language translation example, the recurrent network in the decoder will need the tokenized data both as input and output but shifted by one location and with a START token on the input side. This explains why we provide two variables dest_input_data and dest_target_data to the constructor. We also need to provide the batch size.\n",
    "\n",
    "The __len__() method is expected to provide the number of batches that our dataset provides, which is simply the number of images divided by the batch size.\n",
    "\n",
    "The main functionality in the class is the __getitem__() method, which is expected to return the training data for the batch number indicated by the argument idx. The output format of this method depends on what our network requires as input. For a single training example, our network needs a set of feature vectors as input from the encoder side and a shifted version of the target sentence as input to the decoder recurrent network. It also needs the original version of the target sentence as the desired output for the network. Thus, the output from this method should be a list with two elements representing the two inputs and a single element representing the output. The details become clearer when we later build our training network. There is one more thing to consider, though. The __getitem__() method is expected to return a batch instead of a single training example, so each of the three items we described will be an array where the number of elements is determined by the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence class to create batches on the fly.\n",
    "class ImageCaptionSequence(Sequence):\n",
    "    def __init__(self, image_paths, dest_input_data,\n",
    "                 dest_target_data, batch_size):\n",
    "        self.image_paths = image_paths\n",
    "        self.dest_input_data = dest_input_data\n",
    "        self.dest_target_data = dest_target_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dest_input_data) /\n",
    "            float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x0 = self.image_paths[\n",
    "            idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x1 = self.dest_input_data[\n",
    "            idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.dest_target_data[\n",
    "            idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        image_features = []\n",
    "        for image_id in batch_x0:\n",
    "            file_name = TRAINING_FILE_DIR \\\n",
    "                + image_id + '.pickle.gzip'\n",
    "            pickle_file = gzip.open(file_name, 'rb')\n",
    "            feature_vector = pickle.load(pickle_file)\n",
    "            pickle_file.close()\n",
    "            image_features.append(feature_vector)\n",
    "        return [np.array(image_features),\n",
    "                np.array(batch_x1)], np.array(batch_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor for the ImageCaptionSequence class above assumes that we already have created three arrays with appropriate input data. Two of these arrays (for the recurrent network in the decoder) directly correspond to what we created in the language translation example c14e1_seq2seq_translate. This is shown in the code snippet below, where we also call the constructor for ImageCaptionSequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in\n",
    "                        dest_target_token_seq]\n",
    "dest_input_data = pad_sequences(dest_input_token_seq,\n",
    "                                padding='post')\n",
    "dest_target_data = pad_sequences(\n",
    "    dest_target_token_seq, padding='post',\n",
    "    maxlen=len(dest_input_data[0]))\n",
    "image_sequence = ImageCaptionSequence(\n",
    "    image_paths, dest_input_data, dest_target_data, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define the encoder and decoder models and connect them. For a detailed description, including figures, please consult the description in the book. The architecture is a typical encoder-decoder architecture, although most of the encoding has already been done offline. The code snippet below shows the implementation of the encoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 14, 14, 512  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 512)         0           ['input_1[0][0]']                \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          131328      ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          131328      ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 262,656\n",
      "Trainable params: 262,656\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build encoder model.\n",
    "# Input is feature vector.\n",
    "feature_vector_input = Input(shape=(14, 14, 512))\n",
    "\n",
    "# Create the encoder layers.\n",
    "enc_mean_layer = GlobalAveragePooling2D()\n",
    "enc_layer_h = Dense(LAYER_SIZE)\n",
    "enc_layer_c = Dense(LAYER_SIZE)\n",
    "\n",
    "# Connect the encoding layers.\n",
    "enc_mean_layer_output = enc_mean_layer(feature_vector_input)\n",
    "enc_layer_h_outputs = enc_layer_h(enc_mean_layer_output)\n",
    "enc_layer_c_outputs = enc_layer_c(enc_mean_layer_output)\n",
    "\n",
    "# Organize the output state for encoder layers.\n",
    "enc_layer_outputs = [enc_layer_h_outputs, enc_layer_c_outputs]\n",
    "\n",
    "# Build the model.\n",
    "enc_model_top = Model(feature_vector_input, enc_layer_outputs)\n",
    "enc_model_top.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet implements the decoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"lstm\" (type LSTM).\n\nmodule 'tensorflow.compat.v2.__internal__.function' has no attribute 'defun_with_attributes'\n\nCall arguments received by layer \"lstm\" (type LSTM):\n  • inputs=['tf.Tensor(shape=(None, None, 128), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5490/1063414954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     dec_feature_vector_input)\n\u001b[1;32m     27\u001b[0m \u001b[0mdec_layer1_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_layer1_state_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_layer1_state_c\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     dec_layer1(dec_embedding_layer_outputs, initial_state=[\n\u001b[0m\u001b[1;32m     29\u001b[0m         dec_layer1_state_input_h, dec_layer1_state_input_c])\n\u001b[1;32m     30\u001b[0m \u001b[0mdec_query_layer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_query_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_layer1_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# Perform the call with temporarily replaced input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;31m# Remove the additional_specs from input spec and keep the rest. It\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;31m# is important to keep since the input spec was populated by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/rnn/gru_lstm_utils.py\u001b[0m in \u001b[0;36mgenerate_defun_backend\u001b[0;34m(unique_api_name, preferred_device, func, supportive_attributes)\u001b[0m\n\u001b[1;32m    224\u001b[0m     }\n\u001b[1;32m    225\u001b[0m     \u001b[0mfunction_attributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupportive_attributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     return tf.__internal__.function.defun_with_attributes(\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer \"lstm\" (type LSTM).\n\nmodule 'tensorflow.compat.v2.__internal__.function' has no attribute 'defun_with_attributes'\n\nCall arguments received by layer \"lstm\" (type LSTM):\n  • inputs=['tf.Tensor(shape=(None, None, 128), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "# Build decoder model.\n",
    "# Input to the network is feature_vector, image caption\n",
    "# sequence, and intermediate state.\n",
    "dec_feature_vector_input = Input(shape=(14, 14, 512))\n",
    "dec_embedding_input = Input(shape=(None, ))\n",
    "dec_layer1_state_input_h = Input(shape=(LAYER_SIZE,))\n",
    "dec_layer1_state_input_c = Input(shape=(LAYER_SIZE,))\n",
    "\n",
    "# Create the decoder layers.\n",
    "dec_reshape_layer = Reshape((196, 512),\n",
    "                            input_shape=(14, 14, 512,))\n",
    "dec_attention_layer = Attention()\n",
    "dec_query_layer = Dense(512)\n",
    "dec_embedding_layer = Embedding(output_dim=EMBEDDING_WIDTH,\n",
    "                                input_dim=MAX_WORDS,\n",
    "                                mask_zero=False)\n",
    "dec_layer1 = LSTM(LAYER_SIZE, return_state=True,\n",
    "                  return_sequences=True)\n",
    "dec_concat_layer = Concatenate()\n",
    "dec_layer2 = Dense(MAX_WORDS, activation='softmax')\n",
    "\n",
    "# Connect the decoder layers.\n",
    "dec_embedding_layer_outputs = dec_embedding_layer(\n",
    "    dec_embedding_input)\n",
    "dec_reshape_layer_outputs = dec_reshape_layer(\n",
    "    dec_feature_vector_input)\n",
    "dec_layer1_outputs, dec_layer1_state_h, dec_layer1_state_c = \\\n",
    "    dec_layer1(dec_embedding_layer_outputs, initial_state=[\n",
    "        dec_layer1_state_input_h, dec_layer1_state_input_c])\n",
    "dec_query_layer_outputs = dec_query_layer(dec_layer1_outputs)\n",
    "dec_attention_layer_outputs = dec_attention_layer(\n",
    "    [dec_query_layer_outputs, dec_reshape_layer_outputs])\n",
    "dec_layer2_inputs = dec_concat_layer(\n",
    "    [dec_layer1_outputs, dec_attention_layer_outputs])\n",
    "dec_layer2_outputs = dec_layer2(dec_layer2_inputs)\n",
    "\n",
    "# Build the model.\n",
    "dec_model = Model([dec_feature_vector_input,\n",
    "                   dec_embedding_input,\n",
    "                   dec_layer1_state_input_h,\n",
    "                   dec_layer1_state_input_c],\n",
    "                  [dec_layer2_outputs, dec_layer1_state_h,\n",
    "                   dec_layer1_state_c])\n",
    "dec_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a joint model from the encoder and decoder in the code snippet below. This model will be used for training. Just as in the text translation example, we discard the state outputs from the decoder in this joint model. There is no need for explicit state management for this joint model because TensorFlow does it for us during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile full training model.\n",
    "# We do not use the state output when training.\n",
    "train_feature_vector_input = Input(shape=(14, 14, 512))\n",
    "train_dec_embedding_input = Input(shape=(None, ))\n",
    "intermediate_state = enc_model_top(train_feature_vector_input)\n",
    "train_dec_output, _, _ = dec_model([train_feature_vector_input,\n",
    "                                    train_dec_embedding_input] +\n",
    "                                    intermediate_state)\n",
    "training_model = Model([train_feature_vector_input,\n",
    "                        train_dec_embedding_input],\n",
    "                        [train_dec_output])\n",
    "training_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer='adam', metrics =['accuracy'])\n",
    "training_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for the language translation example, we use the encoder and decoder separately during inference. However, in this image-captioning example, the encoder also needs to include the VGG19 layers, as we will not do inference on precomputed feature vectors. We therefore create yet another model in the code snippet below, which consists of the VGG19 network (except for the top layers) followed by our decoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full encoder model for inference.\n",
    "conv_model = VGG19(weights='imagenet')\n",
    "conv_model_outputs = conv_model.get_layer('block5_conv4').output\n",
    "intermediate_state = enc_model_top(conv_model_outputs)\n",
    "inference_enc_model = Model([conv_model.input],\n",
    "                            intermediate_state\n",
    "                            + [conv_model_outputs])\n",
    "inference_enc_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to train and evaluate our model, and the code is found below. One key difference compared to past code examples is that instead of providing the training set, we provide the image_sequence object as argument to the fit() function. The image_sequence object will provide the training data batch by batch as the feature vectors are read from disk.\n",
    "\n",
    "After each training epoch, we run through our four test images. The process for this is similar to what we did in the language translation example but with one difference. Instead of running an input sentence through the encoder model that was based on a recurrent network, we read an image from disk, preprocess it, and run it through our encoder model that is based on the convolutional VGG19 network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS): # Train and evaluate model\n",
    "    print('step: ' , i)\n",
    "    history = training_model.fit(image_sequence, epochs=1)\n",
    "    for filename in TEST_IMAGES:\n",
    "        # Determine dimensions.\n",
    "        image = load_img(TEST_FILE_DIR + filename)\n",
    "        width = image.size[0]\n",
    "        height = image.size[1]\n",
    "\n",
    "        # Resize so shortest side is 256 pixels.\n",
    "        if height > width:\n",
    "            image = load_img(\n",
    "                TEST_FILE_DIR + filename,\n",
    "                target_size=(int(height/width*256), 256))\n",
    "        else:\n",
    "            image = load_img(\n",
    "                TEST_FILE_DIR + filename,\n",
    "                target_size=(256, int(width/height*256)))\n",
    "        width = image.size[0]\n",
    "        height = image.size[1]\n",
    "        image_np = img_to_array(image)\n",
    "\n",
    "        # Crop to center 224x224 region.\n",
    "        h_start = int((height-224)/2)\n",
    "        w_start = int((width-224)/2)\n",
    "        image_np = image_np[h_start:h_start+224,\n",
    "                            w_start:w_start+224]\n",
    "\n",
    "        # Run image through encoder.\n",
    "        image_np = np.expand_dims(image_np, axis=0)\n",
    "        x = preprocess_input(image_np)\n",
    "        dec_layer1_state_h, dec_layer1_state_c, feature_vector = \\\n",
    "            inference_enc_model.predict(x, verbose=0)\n",
    "\n",
    "        # Predict sentence word for word.\n",
    "        prev_word_index = START_INDEX\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        for j in range(MAX_LENGTH):\n",
    "            x = np.reshape(np.array(prev_word_index), (1, 1))\n",
    "            preds, dec_layer1_state_h, dec_layer1_state_c = \\\n",
    "                dec_model.predict(\n",
    "                    [feature_vector, x, dec_layer1_state_h,\n",
    "                     dec_layer1_state_c], verbose=0)\n",
    "            prev_word_index = np.asarray(preds[0][0]).argmax()\n",
    "            pred_seq.append(prev_word_index)\n",
    "            if prev_word_index == STOP_INDEX:\n",
    "                break\n",
    "        tokens_to_words(dest_tokenizer, pred_seq)\n",
    "        print('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
